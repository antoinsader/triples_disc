{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\Desktop\\uni\\NLP_WIKI\\code\\BRASK\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Data import get_min_descriptionsNorm_triples_relations, LOGGER_FILES, PKLS_FILES\n",
    "from utils.utils import read_cached_array, cache_array, get_logger\n",
    "from transformers import BertTokenizerFast\n",
    "from flashtext import KeywordProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "k = 10\n",
    "DESCRIPTION_MAX_LENGTH = 128\n",
    "descs, triples, relations, aliases = get_min_descriptionsNorm_triples_relations(k)\n",
    "golden_triples_file = PKLS_FILES[\"golden_triples\"][k]\n",
    "len(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_start_end_idxs(entity_description_tokens, entity_to_find_aliases, tokenizer):\n",
    "    \"\"\"\n",
    "        I try to find using flashtext the entity to find in the description, if found return the idxs \n",
    "    \"\"\" \n",
    "    spans = []\n",
    "    alias_token_lists = []\n",
    "    for alias in entity_to_find_aliases:\n",
    "        toks = tokenizer.tokenize(alias)\n",
    "        alias_token_lists.append((toks, alias))\n",
    "    \n",
    "    L = len(entity_description_tokens)\n",
    "    for toks, alias in alias_token_lists:\n",
    "        m = len(toks)\n",
    "        if m == 0 or m > L:\n",
    "            continue\n",
    "        for i in range(0, L - m + 1):\n",
    "            if entity_description_tokens[i : i + m] == toks:\n",
    "                spans.append((i, i + m - 1, alias))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array chached in file ./data/dictionaries/golden_triples/golden_triples_min_10.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "buid_logger = get_logger(\"build_golden_triples\", LOGGER_FILES[\"build_golden_triples\"])\n",
    "\n",
    "sentences_ids = list(descs.keys())\n",
    "sentences = list(descs.values())\n",
    "encoded = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=DESCRIPTION_MAX_LENGTH)\n",
    "sentences_tokens = [tokenizer.convert_ids_to_tokens(sen) for sen in encoded[\"input_ids\"]]\n",
    "\n",
    "golden_triples = []\n",
    "buid_logger.info(f\"Building golden triples for {len(descs)}: \\n We will log every description sentence and then the aliases for the head, tail of the description triples and log the extracted golden triples.. By triples I mean double of (head, tail)\")\n",
    "\n",
    "\n",
    "for sen_tokens, sen_id in zip(sentences_tokens, sentences_ids):\n",
    "    buid_logger.info(f\"\\n\\nSentence : {descs[sen_id]}\")\n",
    "    buid_logger.info(f\"Sentence tokens: {sen_tokens}\")\n",
    "    sen_trps = []\n",
    "    trpls = triples[sen_id]\n",
    "    triples_found_logs = []\n",
    "    for h, r, t in trpls:\n",
    "        buid_logger.info(f\"\\n\\t next triple: \")\n",
    "        h_aliases = aliases[h]\n",
    "        buid_logger.info(f\"\\thead aliases: {h_aliases}\")\n",
    "        trp_h = get_entity_start_end_idxs(sen_tokens, h_aliases, tokenizer)\n",
    "        if len(trp_h) == 0:\n",
    "            buid_logger.info(f\"\\t \\t no head aliases matched with the sentence\")\n",
    "            continue\n",
    "        buid_logger.info(f\"\\t \\thead aliases matched {trp_h}\")\n",
    "        \n",
    "        t_aliases = aliases[t]\n",
    "        buid_logger.info(f\"\\ttail aliases: {t_aliases}\")\n",
    "        trp_t = get_entity_start_end_idxs(sen_tokens, t_aliases, tokenizer)\n",
    "        \n",
    "        if len(trp_t) == 0:\n",
    "            buid_logger.info(f\"\\t \\tno tail aliases matched with the sentence\")\n",
    "            continue \n",
    "        buid_logger.info(f\"\\t \\ttail aliases matched {trp_t}\")\n",
    "        \n",
    "        relation_aliases = relations[r]\n",
    "        for matched_head in trp_h:\n",
    "            for matched_tail in trp_t:\n",
    "                sen_trps.append(( (matched_head[0], matched_head[1]) ,  (matched_tail[0], matched_tail[1])  ))\n",
    "                triples_found_logs.append(f\"\\n\\n\\t******** TRIPLE FOUND:  (HEAD, TAIL): ({matched_head[2]}, {matched_tail[2]}), (head_idxs, tail_idxs): ({(matched_head[0], matched_head[1]) ,  (matched_tail[0], matched_tail[1])}) \\n\\tRelation aliases: {relation_aliases}\"   )\n",
    "                \n",
    "    for l in triples_found_logs:\n",
    "        buid_logger.info(l)\n",
    "               \n",
    "    golden_triples.append(sen_trps)\n",
    "\n",
    "\n",
    "\n",
    "cache_array(golden_triples, golden_triples_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install flashtext\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "tail_aliases = ['sexy ann', 'Sexy Ann']\n",
    "\n",
    "sentence = 'System in Blue\" is a song by German music group Systems in Blue, released as promo single in 2005. it was produced by Rolf Kohler, Detlef Wiedeke and Thomas Widrat.it was re-released in 2009 by Akasa Records, containing an extended version produced by Talking System.'\n",
    "# 1) build your keyword‐processor\n",
    "kp = KeywordProcessor(case_sensitive=False, )\n",
    "for alias in tail_aliases:\n",
    "    kp.add_keyword(alias)\n",
    "\n",
    "# 2) extract all matches with their char spans\n",
    "matches = kp.extract_keywords(sentence, span_info=True)\n",
    "# matches is a list of tuples: [(keyword, start_char, end_char), …]\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_id, d_txt in descs.items():\n",
    "    triples = triples[d_id]\n",
    "    for h, _, t in triples:\n",
    "        \n",
    "    break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
