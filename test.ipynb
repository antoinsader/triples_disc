{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\Desktop\\uni\\NLP_WIKI\\code\\BRASK\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Data import get_min_descriptionsNorm_triples_relations, LOGGER_FILES, PKLS_FILES\n",
    "from utils.utils import read_cached_array, cache_array, get_logger\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def batch_tokenize(sentences, tokenizer, batch_size=2056):\n",
    "    all_batches = []\n",
    "    all_masks = []\n",
    "    sentence_tokens = []\n",
    "    \n",
    "    total_batches = ceil(len(sentences) / batch_size)\n",
    "    for i in tqdm(\n",
    "        range(0, len(sentences), batch_size),\n",
    "        total=total_batches,\n",
    "        desc=\"Tokenizing batches\",\n",
    "        unit=\"batch\"\n",
    "    ):\n",
    "        batch = sentences[i : i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, \n",
    "            return_offsets_mapping=True, \n",
    "            add_special_tokens = False \n",
    "        )\n",
    "        \n",
    "        \n",
    "        sentence_tokens.extend([tokenizer.convert_ids_to_tokens(sen) for sen in enc[\"input_ids\"]])\n",
    "    return torch.cat(all_batches, dim=0), torch.cat(all_masks, dim=0) , sentence_tokens\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_descriptions(sentences_texts):\n",
    "    \n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    descs_tokenized_input_ids, descs_tokenized_attention_masks, sentence_tokens = batch_tokenize(sentences_texts, tokenizer, batch_size=10)\n",
    "    return descs_tokenized_input_ids, descs_tokenized_attention_masks, sentence_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 \n",
    "CHUNK_SIZE = 2\n",
    "DESCRIPTION_MAX_LENGTH = 128\n",
    "\n",
    "descs, triples, relations, aliases = get_min_descriptionsNorm_triples_relations(k)\n",
    "sentences_ids = list(descs.keys())\n",
    "sentences_texts = list(descs.values())\n",
    "\n",
    "# descs_tokenized_input_ids, descs_tokenized_attention_masks, sentence_tokens = tokenize_descriptions(sentences_texts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases = {\n",
    "    \"q100\": [\"football\", \"football soccer\"],\n",
    "    \"q500\": [\"number 9\", \"num 0\"],\n",
    "    \"q700\": [\"Roland crystal\"],\n",
    "    \"q800\": [\"ML\", \"machine learning\"],\n",
    "    \"q900\": [\"italy\", \"italia\"],\n",
    "    \"q901\": [\"europe\", \"european union\"],\n",
    "    \"q1000\": [\"soccer football player\"]\n",
    "}\n",
    "triples= {\n",
    "    \"q2\": [ (\"q700\", \"r2\", \"q800\")],\n",
    "    \"q1\": [ (\"q100\" , \"r1\", \"q500\"), (\"q100\", \"r2\", \"q1000\")],\n",
    "    \"q3\": [(\"q900\", \"r3\", \"q901\")]\n",
    "}\n",
    "\n",
    "descriptions = {\n",
    "    \"q1\": \"Raymond Neifel is an indian football soccer player with number 9\",\n",
    "    \"q2\": \"Roland Crystal is the greatest machine learning engineer\",\n",
    "    \"q3\": \"Italia is a country in the european union\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_triples_heads_aliases : [[['football', 'football soccer'], ['football', 'football soccer']], [['Roland crystal']], [['italy', 'italia']]]\n",
      "sentences_triples_tails_aliases : [[['number 9', 'num 0'], ['soccer football player']], [['ML', 'machine learning']], [['europe', 'european union']]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'football': re.compile(r'football', re.IGNORECASE|re.UNICODE),\n",
       " 'football soccer': re.compile(r'football\\s*soccer', re.IGNORECASE|re.UNICODE),\n",
       " 'number 9': re.compile(r'number\\s*9', re.IGNORECASE|re.UNICODE),\n",
       " 'num 0': re.compile(r'num\\s*0', re.IGNORECASE|re.UNICODE),\n",
       " 'Roland crystal': re.compile(r'Roland\\s*crystal', re.IGNORECASE|re.UNICODE),\n",
       " 'ML': re.compile(r'ML', re.IGNORECASE|re.UNICODE),\n",
       " 'machine learning': re.compile(r'machine\\s*learning',\n",
       "            re.IGNORECASE|re.UNICODE),\n",
       " 'italy': re.compile(r'italy', re.IGNORECASE|re.UNICODE),\n",
       " 'italia': re.compile(r'italia', re.IGNORECASE|re.UNICODE),\n",
       " 'europe': re.compile(r'europe', re.IGNORECASE|re.UNICODE),\n",
       " 'european union': re.compile(r'european\\s*union', re.IGNORECASE|re.UNICODE),\n",
       " 'soccer football player': re.compile(r'soccer\\s*football\\s*player',\n",
       "            re.IGNORECASE|re.UNICODE)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "sentences_triples_heads_aliases = [\n",
    "    [aliases[t[0]] for t in triples[s]] \n",
    "    for s in sentences_ids\n",
    "]\n",
    "\n",
    "sentences_triples_tails_aliases = [\n",
    "    [aliases[t[2]] for t in triples[s]] \n",
    "    for s in sentences_ids\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"sentences_triples_heads_aliases : {sentences_triples_heads_aliases}\")\n",
    "print(f\"sentences_triples_tails_aliases : {sentences_triples_tails_aliases}\")\n",
    "\n",
    "alias_pattern_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1328.78batch/s]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "CHUNK_SIZE = 2\n",
    "BATCH_SIZE = len(descriptions)\n",
    "L = 128\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "sentences_ids = list(descriptions.keys())\n",
    "sentences_texts = list(descriptions.values())\n",
    "\n",
    "\n",
    "sentences_triples_heads_aliases = [\n",
    "    [aliases[t[0]] for t in triples[s]] \n",
    "    for s in sentences_ids\n",
    "]\n",
    "sentences_triples_tails_aliases = [\n",
    "    [aliases[t[2]] for t in triples[s]] \n",
    "    for s in sentences_ids\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "alias_pattern_map = {} \n",
    "for lst in aliases.values():\n",
    "    for alias in lst:\n",
    "        escaped = re.escape(alias)\n",
    "        flexible  = escaped.replace(r\"\\ \", r\"\\s*\")\n",
    "        pattern   = rf\"\\b{flexible}\\b\"\n",
    "        alias_pattern_map[alias] = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "silver_span_head_s = torch.zeros(BATCH_SIZE, L )\n",
    "silver_span_head_e = torch.zeros(BATCH_SIZE, L )\n",
    "silver_span_tail_s = torch.zeros(BATCH_SIZE, L )\n",
    "silver_span_tail_e = torch.zeros(BATCH_SIZE, L )\n",
    "\n",
    "all_sentences_tokens = []\n",
    "all_sentences_offsets = []\n",
    "\n",
    "\n",
    "total_batches = ceil(len(sentences_texts) / CHUNK_SIZE)\n",
    "for i in tqdm(\n",
    "    range(0, len(sentences_texts), CHUNK_SIZE),\n",
    "    total=total_batches,\n",
    "    desc=\"Tokenizing batches\",\n",
    "    unit=\"batch\"\n",
    "):\n",
    "\n",
    "    batch = sentences_texts[i : i + CHUNK_SIZE]\n",
    "    enc = tokenizer(\n",
    "        batch, \n",
    "        return_offsets_mapping=True, \n",
    "        add_special_tokens = False \n",
    "    )\n",
    "    all_sentences_offsets.extend(enc.offset_mapping)\n",
    "    \n",
    "    for sen_idx, enc_obj in enumerate(enc.encodings):\n",
    "        all_sentences_tokens.append(enc_obj.tokens)\n",
    "        \n",
    "        sentence_idx_in_batch = i + sen_idx\n",
    "        current_description = sentences_texts[sentence_idx_in_batch]\n",
    "        sentence_heads_aliases = sentences_triples_heads_aliases[sentence_idx_in_batch]\n",
    "        sentence_tails_aliases = sentences_triples_tails_aliases[sentence_idx_in_batch]\n",
    "        sentence_tokens_offset = all_sentences_offsets[sentence_idx_in_batch]\n",
    "        \n",
    "        for one_als_list in sentence_heads_aliases:\n",
    "            for als_str in one_als_list:\n",
    "                pattern = alias_pattern_map[als_str]  \n",
    "                m = pattern.search(current_description)\n",
    "                if not m: continue \n",
    "                start_char, end_char = m.span()\n",
    "                token_indices = [\n",
    "                    i for i, (s, e) in enumerate(sentence_tokens_offset)\n",
    "                    if (s < end_char) and (e > start_char)\n",
    "                ]\n",
    "                head_start, head_end = token_indices[0], token_indices[-1]\n",
    "                silver_span_head_s[sentence_idx_in_batch, head_start] = 1\n",
    "                silver_span_head_e[sentence_idx_in_batch, head_end] = 1\n",
    "                break\n",
    "        \n",
    "        for one_als_list in sentence_tails_aliases:\n",
    "            for als_str in one_als_list:\n",
    "                pattern = alias_pattern_map[als_str]  \n",
    "                m = pattern.search(current_description)\n",
    "                if not m: continue \n",
    "                start_char, end_char = m.span()\n",
    "                token_indices = [\n",
    "                    i for i, (s, e) in enumerate(sentence_tokens_offset)\n",
    "                    if (s < end_char) and (e > start_char)\n",
    "                ]\n",
    "                tail_start, tail_end = token_indices[0], token_indices[-1]\n",
    "                silver_span_tail_s[sentence_idx_in_batch, tail_start] = 1\n",
    "                silver_span_tail_e[sentence_idx_in_batch, tail_end] = 1\n",
    "                break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  Raymond Neifel is an indian football soccer player with number 9\n",
      "\t HEAD: ['football']\n",
      "sentence:  Roland Crystal is the greatest machine learning engineer\n",
      "\t HEAD: ['Roland', 'Crystal']\n",
      "sentence:  Italia is a country in the european union\n",
      "\t HEAD: ['Italia']\n",
      "sentence:  Raymond Neifel is an indian football soccer player with number 9\n",
      "\t TAIL: ['number', '9']\n",
      "sentence:  Roland Crystal is the greatest machine learning engineer\n",
      "\t TAIL: ['machine', 'learning']\n",
      "sentence:  Italia is a country in the european union\n",
      "\t TAIL: ['euro', '##pe']\n"
     ]
    }
   ],
   "source": [
    "for sen_idx in range(BATCH_SIZE):\n",
    "    head_starts = silver_span_head_s[sen_idx]\n",
    "    head_ends = silver_span_head_e[sen_idx]\n",
    "    sen_tokens = all_sentences_tokens[sen_idx]\n",
    "    \n",
    "    head_starts_idxs = torch.nonzero(head_starts == 1, as_tuple=True)[0]\n",
    "    head_ends_idxs = torch.nonzero(head_ends == 1, as_tuple=True)[0]\n",
    "    \n",
    "    print(f\"sentence: \" , sentences_texts[sen_idx])\n",
    "    for h_s, h_e in zip(head_starts_idxs, head_ends_idxs):\n",
    "        print(f\"\\t HEAD: {sen_tokens[h_s: h_e + 1]}\")\n",
    "        \n",
    "\n",
    "for sen_idx in range(BATCH_SIZE):\n",
    "    tail_starts = silver_span_tail_s[sen_idx]\n",
    "    tail_ends = silver_span_tail_e[sen_idx]\n",
    "    sen_tokens = all_sentences_tokens[sen_idx]\n",
    "    \n",
    "    tail_starts_idxs = torch.nonzero(tail_starts == 1, as_tuple=True)[0]\n",
    "    tail_ends_idxs = torch.nonzero(tail_ends == 1, as_tuple=True)[0]\n",
    "    \n",
    "    print(f\"sentence: \" , sentences_texts[sen_idx])\n",
    "    for t_s, t_e in zip(tail_starts_idxs, tail_ends_idxs):\n",
    "        print(f\"\\t TAIL: {sen_tokens[t_s: t_e + 1]}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK MY SILVER SPANS THAT I CREATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_spans = read_cached_array(PKLS_FILES[\"silver_spans\"][1000])\n",
    "\n",
    "silver_span_head_s = silver_spans[\"head_start\"]\n",
    "silver_span_head_e = silver_spans[\"head_end\"]\n",
    "silver_span_tail_s = silver_spans[\"tail_start\"]\n",
    "silver_span_tail_e = silver_spans[\"tail_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all count: 3364\n",
      "has triples count: 575\n"
     ]
    }
   ],
   "source": [
    "print(f\"all count: {silver_span_head_s.shape[0]}\")\n",
    "has_triples = 0\n",
    "for idx, (h_s, h_e, t_s, t_e) in enumerate(zip(silver_span_head_s, silver_span_head_e, silver_span_tail_s, silver_span_tail_e)):\n",
    "    idxs_1 = torch.nonzero(h_s == 1, as_tuple=True)[0]\n",
    "    idxs_2 = torch.nonzero(h_e == 1, as_tuple=True)[0]\n",
    "    idxs_3 = torch.nonzero(t_s == 1, as_tuple=True)[0]\n",
    "    idxs_4 = torch.nonzero(t_s == 1, as_tuple=True)[0]\n",
    "    if len(idxs_1) == 0 or len(idxs_2) == 0 or len(idxs_3) == 0 or len(idxs_4) == 0: continue\n",
    "    has_triples += 1\n",
    "    \n",
    "print(f\"has triples count: {has_triples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my sentence text: Good Deal with Dave Lieberman is a television cooking show hosted by Dave Lieberman that airs on the Food Network in the United States and Food Network Canada in Canada. The show premiered on Food Network on April 16, 2005. Lieberman's show presents affordable gourmet quality recipes.\n",
      "my_sentence_tokens: ['Good', 'Deal', 'with', 'Dave', 'Lie', '##berman', 'is', 'a', 'television', 'cooking', 'show', 'hosted', 'by', 'Dave', 'Lie', '##berman', 'that', 'airs', 'on', 'the', 'Food', 'Network', 'in', 'the', 'United', 'States', 'and', 'Food', 'Network', 'Canada', 'in', 'Canada', '.', 'The', 'show', 'premiered', 'on', 'Food', 'Network', 'on', 'April', '16', ',', '2005', '.', 'Lie', '##berman', \"'\", 's', 'show', 'presents', 'affordable', 'go', '##ur', '##met', 'quality', 'recipes', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "sentence triples: \n",
      "\t NEW TRIPLE\n",
      "\t head: ['good deal with dave lieberman']\n",
      "\t tail: ['english lexicon', 'english langauge', 'english language 1', 'number of english words', 'english the global language', 'en', 'english-language debut', 'anglesc', 'iso 639:en', 'geographic distribution of english', 'english (langauge)', 'english speakers', 'non-english', 'english-speaker', 'english vocabulary', 'english language.', 'books/english language', 'the english langauge', 'englishlanguage', 'english speaking', 'english vernacular', 'saozneg', 'vernacular english', 'sudanese english', 'english-language', 'english language', 'number of words in english', 'the english language', 'english language learning and teaching', 'englische sprache', 'english languge', 'iso 639:eng', 'geographical distribution of english', 'english medium', 'number of words in the english language', 'english (language)', 'english language word count', 'spoken english']\n",
      "\t NEW TRIPLE\n",
      "\t head: ['good deal with dave lieberman']\n",
      "\t tail: ['foodnetwork', 'foodtv', 'united states food network', 'tv food network', '@foodnetwork', 'foodnetwork.com', 'ice brigade', 'cooks vs. cons', 'the food network', 'the fn dish', 'food network (us)', 'television food network', 'food network', 'food network hd']\n",
      "\t NEW TRIPLE\n",
      "\t head: ['good deal with dave lieberman']\n",
      "\t tail: ['production season', 'television show', 'cour (tv production)', 'television segment', 'making televsion', 'television program', 'tv-show', 'split television season', 'television programmes', 'tv show', 'tv program', 'television shows', 'television programs', 'television production', 'original television series', 'limited series (television)', 'scripted television', 'making television', 'tv-program', 'television programme', 'televison series', 'television serie', 'tv programmes', 'television serials', 'tv broadcast', 'television broadcast', 'series television', 'tv programme', 'tv series', 'television series', 'program (tv)', 'television show/comments', 'back-nine-order', 'tv-series', 'television season', 'season (television)', 'tv programs', 'fernsehserien']\n",
      "\t NEW TRIPLE\n",
      "\t head: ['good deal with dave lieberman']\n",
      "\t tail: ['etats-unis', 'estados unidos de amÃ©rica', 'untied states', 'unitd states', 'vereinigte staaten', \"stati uniti d'america\", \"etats-unis d'amerique\", 'america', 'v.s. america', 'estados unidos de america', 'us and a', 'u.s', 'nagkaisang mga estado', 'Ã©tats-unis', 'unites states', 'united states of america/introduction', 'united states/references', 'america (united states)', 'v.s. amerika', 'united-states', 'u.s. america', 'united+states', 'united satates', 'untied states of america', 'usofa', 'ee. uu.', 'u.s. of a', 'u. s. a.', 'amercia', 'estados unidos', 'u.s.american', '(us)', 'estatos unitos', \"Ã©tats-unis d'amÃ©rique\", 'united states of america/oldpage', 'united states portal', 'columbian union', 'usa/doc', 'united sates', 'ee uu', 'u.s. of america', 'us of a', 'united+states+of+america', 'u s', 'america (united states of)', 'the united states of america.', 'name of the united states', 'amurika', 'us of america', 'america (us)', 'united states (of america)', 'los estados unidos de amÃ©rica', 'us', 'unitedstates', 'iso 3166-1:us', 'usa/sandbox', 'u. s. of a.', 'vs amerika', 'america, united states of', 'p:us', 'united states (u.s.a.)', 'u,s,', 'united stated', 'amarica', '(usa)', 'federal united states', 'the unites states of america', 'united state of america', 'u.s.', 'u.s. of a.', 'eeuu', 'us (country)', 'united staets of america', 'the us of america', 'united states', 'ee.uu.', 'united states of amerca', 'etymology of the united states', 'the usa', 'united states of america (usa)', 'unite states of america', 'unite states', 'american united states', 'americaland', 'unietd states', 'united states of america', 'u s a', 'the u.s.', 'los estados unidos', 'u.s.a.)', 'the american united states', 'vs america', 'u.s.a.', 'america (usa)', 'these united states of america', 'nited states', 'civitates foederatae americae', 'united states of american', 'united states of america (u.s.a.)', 'united states of america (redirect)', 'the us', 'usa', 'u.s.a', 'united states (u.s.)', 'u. s.', 'the states', 'p:usa', 'united states america', 'amurica', 'americia', 'the us of a', 'united states (country)', 'the u. states of america', 'america (country)', 'amerka', 'ÑÐ¾ÐµÐ´Ð¸Ð½Ñ‘Ð½Ð½Ñ‹Ðµ ÑˆÑ‚Ð°Ñ‚Ñ‹ Ð°Ð¼ÐµÑ€Ð¸ÐºÐ¸', 'u.s. a', 'us america', 'the united states of america', 'ðŸ‡ºðŸ‡¸', 'united states (us)', 'united states/introduction', 'the united states', 'united states of america.', 'us.', 'usa portal']\n",
      "silver spans\n",
      "tensor([19, 24])\n",
      "tensor([21, 25])\n",
      "HEAD:  ['Good', 'Deal', 'with', 'Dave', 'Lie', '##berman'] \n",
      "TAIL:  ['the', 'Food', 'Network'] \n",
      "TAIL:  ['United', 'States'] \n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "k = 1000\n",
    "descs, triples, _, aliases = get_min_descriptionsNorm_triples_relations(k)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "sentences_ids = list(descs.keys())\n",
    "sentences_texts = list(descs.values())\n",
    "\n",
    "my_sentence_id = sentences_ids[idx]\n",
    "my_sentence_text = sentences_texts[idx]\n",
    "my_sentence_triples = triples[my_sentence_id  ]\n",
    "my_sentence_triples_str = [\n",
    "    {\"head\": aliases[trp[0]], \"tail\": aliases[trp[2]]}\n",
    "    for trp in my_sentence_triples\n",
    "]\n",
    "enc = tokenizer(\n",
    "            my_sentence_text, \n",
    "            return_offsets_mapping=True,\n",
    "            add_special_tokens = False,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "my_sentence_tokens = enc[0].tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"my sentence text: {my_sentence_text}\" )\n",
    "print(f\"my_sentence_tokens: {my_sentence_tokens}\" )\n",
    "print(\"sentence triples: \")\n",
    "for tt in my_sentence_triples_str:\n",
    "    print(f\"\\t NEW TRIPLE\")\n",
    "    print(f\"\\t head: {tt['head']}\")\n",
    "    print(f\"\\t tail: {tt['tail']}\")\n",
    "\n",
    "print(\"silver spans\")\n",
    "head_start_idxs = torch.nonzero(silver_span_head_s[idx] == 1, as_tuple=True)[0]\n",
    "head_end_idxs = torch.nonzero(silver_span_head_e[idx] == 1, as_tuple=True)[0]\n",
    "tail_start_idxs = torch.nonzero(silver_span_tail_s[idx] == 1, as_tuple=True)[0]\n",
    "tail_end_idxs = torch.nonzero(silver_span_tail_e[idx] == 1, as_tuple=True)[0]\n",
    "print(tail_start_idxs)\n",
    "print(tail_end_idxs)\n",
    "used_ends = set()\n",
    "for h_s in head_start_idxs:\n",
    "    for h_e in head_end_idxs:\n",
    "        if h_e.item() not in used_ends:\n",
    "            used_ends.add(h_e.item())\n",
    "            print(f\"HEAD:  {my_sentence_tokens[h_s: h_e + 1 ]} \")\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "used_ends = set()\n",
    "for h_s in tail_start_idxs:\n",
    "    for h_e in tail_end_idxs:\n",
    "        if h_e.item() not in used_ends:\n",
    "            used_ends.add(h_e.item())\n",
    "            found_tail = True \n",
    "            print(f\"TAIL:  {my_sentence_tokens[h_s: h_e + 1 ]} \")\n",
    "            \n",
    "            break        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMULATE MY MODEL AND COMPARE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 25])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "idx = 4\n",
    "silver_spans = read_cached_array(PKLS_FILES[\"silver_spans\"][1000])\n",
    "\n",
    "silver_span_head_s = silver_spans[\"head_start\"]\n",
    "silver_span_head_e = silver_spans[\"head_end\"]\n",
    "silver_span_tail_s = silver_spans[\"tail_start\"]\n",
    "silver_span_tail_e = silver_spans[\"tail_end\"]\n",
    "\n",
    "\n",
    "head_start_idxs = torch.nonzero(silver_span_head_s[idx] == 1, as_tuple=True)[0]\n",
    "head_end_idxs = torch.nonzero(silver_span_head_e[idx] == 1, as_tuple=True)[0]\n",
    "tail_start_idxs = torch.nonzero(silver_span_tail_s[idx] == 1, as_tuple=True)[0]\n",
    "tail_end_idxs = torch.nonzero(silver_span_tail_e[idx] == 1, as_tuple=True)[0]\n",
    "\n",
    "tail_end_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((0, 5), 0, (19, 21)), ((0, 5), 0, (24, 25)), ((0, 5), 0, (30, 30))]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from utils.model_helpers import extract_triples\n",
    "\n",
    "sentences = [\n",
    "    \"Good Deal with Dave Lieberman is a television cooking show hosted by Dave Lieberman that airs on the Food Network in the United States and Food Network Canada in Canada. The show premiered on Food Network on April 16, 2005. Lieberman's show presents affordable gourmet quality recipes.\"\n",
    "]\n",
    "L = 128 \n",
    "B = 1\n",
    "R = 1\n",
    "#B, L\n",
    "subj_idxs = [\n",
    "    [(0,5)]\n",
    "]\n",
    "\n",
    "subj_start_probs = torch.zeros((B ,L)) \n",
    "subj_start_probs[0, 0 ] = 0.9\n",
    "\n",
    "subj_end_probs = torch.zeros((B ,L))\n",
    "subj_end_probs[0, 5] = 0.9\n",
    "\n",
    "f_obj_start_probs = torch.zeros((B,L))\n",
    "f_obj_end_probs = torch.zeros((B,L))\n",
    "f_obj_start_probs[0, 19 ] = 0.9\n",
    "f_obj_start_probs[0, 24 ] = 0.9\n",
    "f_obj_start_probs[0, 30 ] = 0.9\n",
    "\n",
    "f_obj_end_probs[0, 21 ] = 0.9\n",
    "f_obj_end_probs[0, 25 ] = 0.9\n",
    "f_obj_end_probs[0, 30 ] = 0.9\n",
    "\n",
    "torch.nonzero(f_obj_start_probs[0] > 0, as_tuple=True)[0]\n",
    "\n",
    "\n",
    "extracted_triples = extract_triples(subj_idxs, f_obj_start_probs.unsqueeze(-1), f_obj_end_probs.unsqueeze(-1),  True , threshold=.5)\n",
    "# B , NUM_TRIPLES\n",
    "len(extracted_triples[0])\n",
    "extracted_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_descriptions_dict(descriptions_dict, silver_spans):\n",
    "    all_keys = list(descriptions_dict.keys())\n",
    "    silver_span_head_s = silver_spans[\"head_start\"]\n",
    "    silver_span_head_e = silver_spans[\"head_end\"]\n",
    "    silver_span_tail_s = silver_spans[\"tail_start\"]\n",
    "    silver_span_tail_e = silver_spans[\"tail_end\"]\n",
    "    \n",
    "    m1 = silver_span_head_s .any(dim=1) \n",
    "    m2 = silver_span_head_e .any(dim=1)\n",
    "    m3 = silver_span_tail_s .any(dim=1) \n",
    "    m4 = silver_span_tail_e .any(dim=1)\n",
    "    mask = m1 & m2 & m3 & m4\n",
    "    indexes    = torch.nonzero(mask, as_tuple=True)[0]\n",
    "    cleaned_desc_dict= {k: descriptions_dict[k]  for idx, k in enumerate(all_keys) if idx in indexes  }\n",
    "    return cleaned_desc_dict, silver_span_head_s[indexes],silver_span_head_e[indexes],silver_span_tail_s[indexes], silver_span_tail_e[indexes]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_helpers import get_h_gs, extract_first_embeddings,extract_last_idxs, extract_triples,  merge_triples\n",
    "\n",
    "\n",
    "class BRASKDataSet(Dataset):\n",
    "    def __init__(self, descriptions_dict, silver_spans , desc_max_length=4):\n",
    "        print(\"Initiating dataset.. \")\n",
    "        cleaned_descriptions, silver_span_head_s, silver_span_head_e, silver_span_tail_s, silver_span_tail_e = descriptions_dict, silver_spans[\"head_start\"], silver_spans[\"head_end\"], silver_spans[\"tail_start\"], silver_spans[\"tail_end\"]\n",
    "        \n",
    "        #silver_spans should be dictionary having keys head_start, head_end, tail_start, tail_end, each one is tensor with shape (B, seq_len) \n",
    "        valid = (len(cleaned_descriptions), desc_max_length) == silver_span_head_s.shape == silver_span_tail_s.shape == silver_span_tail_e.shape==silver_span_head_e.shape \n",
    "        assert valid \n",
    "        if valid:\n",
    "            print(\"\\tvalid\")\n",
    "            tokenizer =BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            model = BertModel.from_pretrained('bert-base-cased')\n",
    "            print(\"\\tcreating clean descriptions\")\n",
    "            \n",
    "            print(f\"\\twe have {len(cleaned_descriptions)} descriptions\")\n",
    "            \n",
    "            sentences = list(cleaned_descriptions.values())\n",
    "            print(\"\\tcreating h_gs\")\n",
    "            self.h_gs, self.embs = get_h_gs(sentences, tokenizer, model, max_length=desc_max_length  )  #  h_gs (batch_size, hidden_size), embs (batch_size, seq_len, hidden_size)\n",
    "            print(f\"\\tself embs shape: {self.embs.shape} should be ({len(cleaned_descriptions)}, {desc_max_length},hidden_size )\")\n",
    "            assert self.embs.shape[0] == len(cleaned_descriptions)\n",
    "            assert self.embs.shape[1] == desc_max_length\n",
    "            self.labels_head_start, self.labels_head_end, self.labels_tail_start, self.labels_tail_end =  silver_span_head_s, silver_span_head_e, silver_span_tail_s, silver_span_tail_e \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return  {\n",
    "            \"h_gs\": self.h_gs[idx], \n",
    "            \"embs\": self.embs[idx],\n",
    "            \"labels_head_start\": self.labels_head_start[idx] ,\n",
    "            \"labels_head_end\": self.labels_head_end[idx],\n",
    "            \"labels_tail_start\": self.labels_tail_start[idx],\n",
    "            \"labels_tail_end\": self.labels_tail_end[idx],\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.h_gs.shape[0]\n",
    "    \n",
    "    def save(self, path):\n",
    "        di = {\n",
    "            \"h_gs\": self.h_gs.cpu(),\n",
    "            \"embs\": self.embs.cpu(),\n",
    "            \"labels_head_start\": self.labels_head_start.cpu() ,\n",
    "            \"labels_head_end\": self.labels_head_end.cpu(),\n",
    "            \"labels_tail_start\":  self.labels_tail_start.cpu(),\n",
    "            \"labels_tail_end\":  self.labels_tail_end.cpu(),\n",
    "            \n",
    "        }\n",
    "        cache_array(di, path)\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        print(\"loadding dataset from cache.. \")\n",
    "        data = read_cached_array(path)\n",
    "\n",
    "        dataset = cls.__new__(cls)\n",
    "        dataset.h_gs = data[\"h_gs\"]\n",
    "        print(f\"\\t dataset.h_gs.shape: {dataset.h_gs.shape}\")\n",
    "        dataset.embs = data[\"embs\"]\n",
    "        print(f\"\\t dataset.embs.shape: {dataset.embs.shape}\")\n",
    "        dataset.labels_head_start = data[\"labels_head_start\"]\n",
    "        dataset.labels_head_end = data[\"labels_head_end\"]\n",
    "        dataset.labels_tail_start = data[\"labels_tail_start\"]\n",
    "        dataset.labels_tail_end = data[\"labels_tail_end\"]\n",
    "        \n",
    "        return dataset\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating dataset.. \n",
      "\tvalid\n",
      "\tcreating clean descriptions\n",
      "\twe have 4 descriptions\n",
      "\tcreating h_gs\n",
      "\tself embs shape: torch.Size([4, 4, 768]) should be (4, 4,hidden_size )\n"
     ]
    }
   ],
   "source": [
    "descriptions_dict = {\n",
    "    \"q1\": \"I am q1 in the field\",\n",
    "    \"q2\": \"I am q2 fsd dsaonv noifd\", \n",
    "    \"q3\": \"I am q3 hello should be deleted\",\n",
    "    \"q4\": \"I am q4 here but not exists\",\n",
    "}\n",
    "\n",
    "silver_spans = {\n",
    "    \"head_start\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "    \"head_end\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "      \"tail_start\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "    \"tail_end\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "dataset = BRASKDataSet(descriptions_dict,silver_spans )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels_head_start': 1.6666666666666667,\n",
       " 'labels_head_end': 1.6666666666666667,\n",
       " 'labels_tail_start': 1.6666666666666667,\n",
       " 'labels_tail_end': 1.6666666666666667}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_keys = [\n",
    "    'labels_head_start', 'labels_head_end',\n",
    "    'labels_tail_start','labels_tail_end'\n",
    "]\n",
    "pos_counts = {k: 0 for k in label_keys}\n",
    "neg_counts = {k: 0 for k in label_keys}\n",
    "\n",
    "loader = DataLoader(dataset,\n",
    "                batch_size=1,\n",
    "                num_workers=0,\n",
    "                shuffle=False)\n",
    "\n",
    "for batch in loader:\n",
    "    for k in label_keys:\n",
    "        lbl = batch[k]\n",
    "        p = lbl.sum().item() \n",
    "        n = lbl.numel() - p \n",
    "        pos_counts[k] += p\n",
    "        neg_counts[k] += n\n",
    "{\n",
    "    k: (neg_counts[k] / pos_counts[k]) if pos_counts[k] > 0 else 1.0\n",
    "    for k in label_keys\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t cleaning descriptions_dict with size 4 \n",
      "\t and silver spans with shape torch.Size([4, 4])\n",
      "mask before: tensor([ True, False,  True,  True])\n",
      "mask now: tensor([ True, False,  True,  True])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'q1': 'I am q1 in the field',\n",
       "  'q3': 'I am q3 hello should be deleted',\n",
       "  'q4': 'I am q4 here but not exists'},\n",
       " tensor([[1, 1, 1, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         [0, 0, 0, 1]]),\n",
       " tensor([[1, 1, 1, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         [0, 0, 0, 1]]),\n",
       " tensor([[1, 1, 1, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         [0, 0, 0, 1]]),\n",
       " tensor([[1, 1, 1, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         [0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "#chatgpt 4\n",
    "def clean_descriptions_dict_opt(descriptions_dict, silver_spans):\n",
    "    print(f\"\\t cleaning descriptions_dict with size {len(descriptions_dict)} \")\n",
    "    all_keys = list(descriptions_dict.keys())\n",
    "    silver_span_head_s = silver_spans[\"head_start\"]\n",
    "    silver_span_head_e = silver_spans[\"head_end\"]\n",
    "    silver_span_tail_s = silver_spans[\"tail_start\"]\n",
    "    silver_span_tail_e = silver_spans[\"tail_end\"]\n",
    "    print(f\"\\t and silver spans with shape {silver_span_tail_e.shape}\")\n",
    "    \n",
    "    \n",
    "    m1 = silver_span_head_s .any(dim=1) \n",
    "    m2 = silver_span_head_e .any(dim=1)\n",
    "    m3 = silver_span_tail_s .any(dim=1) \n",
    "    m4 = silver_span_tail_e .any(dim=1)\n",
    "    mask_prev = m1 & m2 & m3 & m4\n",
    "    print(f\"mask before: {mask_prev}\")\n",
    "    \n",
    "    mask = (\n",
    "        silver_span_head_s .any(dim=1) &\n",
    "        silver_span_head_e .any(dim=1) &\n",
    "        silver_span_tail_s .any(dim=1) &\n",
    "        silver_span_tail_e .any(dim=1)\n",
    "    )\n",
    "    print(f\"mask now: {mask}\")\n",
    "    #wow! I did not know that I can do this\n",
    "    filtered_dict = {\n",
    "        key: value\n",
    "        for (key, value), keep in zip(descriptions_dict.items(), mask)\n",
    "        if keep.item()\n",
    "    }\n",
    "    idx = mask.nonzero(as_tuple=True)[0] \n",
    "    return (\n",
    "        filtered_dict,\n",
    "        silver_span_head_s[idx],\n",
    "        silver_span_head_e[idx],\n",
    "        silver_span_tail_s[idx],\n",
    "        silver_span_tail_e[idx],\n",
    "    )\n",
    "\n",
    "\n",
    "descriptions_dict = {\n",
    "    \"q1\": \"I am q1 in the field\",\n",
    "    \"q2\": \"I am q2 fsd dsaonv noifd\", \n",
    "    \"q3\": \"I am q3 hello should be deleted\",\n",
    "    \"q4\": \"I am q4 here but not exists\",\n",
    "}\n",
    "\n",
    "silver_spans = {\n",
    "    \"head_start\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "    \"head_end\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "      \"tail_start\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "    \"tail_end\" : torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1], \n",
    "            [0,0,0,0], \n",
    "            [1,0,0,0], \n",
    "            [0,0,0,1],\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clean_descriptions_dict_opt(descriptions_dict, silver_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 192.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "tokenizer =BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": list(descriptions_dict.values())})\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128\n",
    "    )\n",
    "encoded = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "input_ids = torch.tensor(encoded['input_ids'])\n",
    "attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    embeddings = bert_output.last_hidden_state # (batch_size, seq_len ,hidden_size)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sentences_tokens: [['Europe', 'is', 'a', 'continent', 'of', 'the'], ['Italy', 'is', 'a', 'country', 'of', 'Europe']]\n",
      "silver_span_head_s: tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.]])\n",
      "silver_span_head_e: tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.]])\n",
      "silver_span_tail_s: tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "silver_span_tail_e: tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "descriptions = {\n",
    "    \"q1\": \"Europe is a continent of the world\",\n",
    "    \"q2\": \"Italy is a country of Europe\"\n",
    "}\n",
    "\n",
    "aliases = {\n",
    "    \"q1\": [\"europe\", \"oropa\", \"eu\"],\n",
    "    \"q2\": [\"italia\", \"italy\", \"it\"],\n",
    "}\n",
    "triples = {\n",
    "    \"q1\": [],\n",
    "    \"q2\": [ (\"q2\", \"r1\", \"q1\")]\n",
    "}\n",
    "\n",
    "BATCH_SIZE = len(descriptions)\n",
    "sentences_texts = list(descriptions.values())\n",
    "\n",
    "\n",
    "\n",
    "def extract_silver_spans(descs, triples, aliases):\n",
    "    CHUNK_SIZE = 1 \n",
    "    L = DESCRIPTION_MAX_LENGTH = 6 \n",
    "    sentences_ids = list(descs.keys())\n",
    "    sentences_texts = list(descs.values())\n",
    "    sentences_triples_heads_aliases = [\n",
    "        [aliases[t[0]] for t in triples[s]] \n",
    "        for s in sentences_ids\n",
    "    ]\n",
    "    sentences_triples_tails_aliases = [\n",
    "        [aliases[t[2]] for t in triples[s]] \n",
    "        for s in sentences_ids\n",
    "    ]\n",
    "    \n",
    "    alias_pattern_map = {} \n",
    "    for lst in aliases.values():\n",
    "        for alias in lst:\n",
    "            escaped = re.escape(alias)\n",
    "            flexible  = escaped.replace(r\"\\ \", r\"\\s*\")\n",
    "            pattern   = rf\"\\b{flexible}\\b\"\n",
    "            alias_pattern_map[alias] = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "\n",
    "    silver_span_head_s = torch.zeros(BATCH_SIZE, L )\n",
    "    silver_span_head_e = torch.zeros(BATCH_SIZE, L )\n",
    "    silver_span_tail_s = torch.zeros(BATCH_SIZE, L )\n",
    "    silver_span_tail_e = torch.zeros(BATCH_SIZE, L )\n",
    "\n",
    "    all_sentences_tokens = []\n",
    "    all_sentences_offsets = []\n",
    "\n",
    "    total_batches = int(len(sentences_texts) / CHUNK_SIZE)\n",
    "    for i in range(0, len(sentences_texts), CHUNK_SIZE):\n",
    "\n",
    "        batch = sentences_texts[i : i + CHUNK_SIZE]\n",
    "        enc = tokenizer(\n",
    "            batch, \n",
    "            return_offsets_mapping=True,\n",
    "            add_special_tokens = False,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            max_length=DESCRIPTION_MAX_LENGTH\n",
    "            \n",
    "        )\n",
    "        all_sentences_offsets.extend(enc.offset_mapping)\n",
    "\n",
    "        for sen_idx, enc_obj in enumerate(enc.encodings):\n",
    "            all_sentences_tokens.append(enc_obj.tokens)\n",
    "\n",
    "            sentence_idx_in_batch = i + sen_idx\n",
    "            current_description = sentences_texts[sentence_idx_in_batch]\n",
    "            sentence_heads_aliases = sentences_triples_heads_aliases[sentence_idx_in_batch]\n",
    "            sentence_tails_aliases = sentences_triples_tails_aliases[sentence_idx_in_batch]\n",
    "            sentence_tokens_offset = all_sentences_offsets[sentence_idx_in_batch]\n",
    "\n",
    "            for one_als_list in sentence_heads_aliases:\n",
    "                for als_str in one_als_list:\n",
    "                    pattern = alias_pattern_map[als_str]\n",
    "                    m = pattern.search(current_description)\n",
    "                    if not m: continue \n",
    "                    start_char, end_char = m.span()\n",
    "                    token_indices = [\n",
    "                        i for i, (s, e) in enumerate(sentence_tokens_offset)\n",
    "                        if (s < end_char) and (e > start_char)\n",
    "                    ]\n",
    "                    if len(token_indices) > 0:\n",
    "                        head_start, head_end = token_indices[0], token_indices[-1]\n",
    "                        silver_span_head_s[sentence_idx_in_batch, head_start] = 1\n",
    "                        silver_span_head_e[sentence_idx_in_batch, head_end] = 1\n",
    "                        break\n",
    "            \n",
    "\n",
    "            for one_als_list in sentence_tails_aliases:\n",
    "                for als_str in one_als_list:\n",
    "                    pattern =  alias_pattern_map[als_str]\n",
    "                    m = pattern.search(current_description)\n",
    "                    if not m: continue \n",
    "                    start_char, end_char = m.span()\n",
    "                    token_indices = [\n",
    "                        i for i, (s, e) in enumerate(sentence_tokens_offset)\n",
    "                        if (s < end_char) and (e > start_char)\n",
    "                    ]\n",
    "                    if len(token_indices) > 0 :\n",
    "                        tail_start, tail_end = token_indices[0], token_indices[-1]\n",
    "                        silver_span_tail_s[sentence_idx_in_batch, tail_start] = 1\n",
    "                        silver_span_tail_e[sentence_idx_in_batch, tail_end] = 1\n",
    "                        break\n",
    "    \n",
    "    return  silver_span_head_s, silver_span_head_e,  silver_span_tail_s,silver_span_tail_e, all_sentences_tokens\n",
    "\n",
    "silver_span_head_s,silver_span_head_e,silver_span_tail_s,silver_span_tail_e, all_sentences_tokens  = extract_silver_spans(descriptions, triples, aliases)\n",
    "\n",
    "print(f\"all_sentences_tokens: {all_sentences_tokens}\")\n",
    "print(f\"silver_span_head_s: {silver_span_head_s}\")\n",
    "print(f\"silver_span_head_e: {silver_span_head_e}\")\n",
    "print(f\"silver_span_tail_s: {silver_span_tail_s}\")\n",
    "print(f\"silver_span_tail_e: {silver_span_tail_e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
